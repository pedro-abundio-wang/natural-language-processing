# Natural Language Processing with Deep Learning

## Word Vectors

300 features is what Google used in their published model trained on the Google news dataset (you can download it from [here](https://code.google.com/archive/p/word2vec/)). The number of features is a "hyper parameter" that you would just have to tune to your application (that is, try different values and see what yields the best results).

### Word2Vec

#### Papers

[Bengio et al., 2003] A neural probabilistic language model.

[Collobert et al., 2011] Natural language processing (almost) from scratch.

[Mikolov et al., 2013] Efficient estimation of word representations in vector space. (original word2vec paper)

[Rong et al., 2014] word2vec parameter learning explained.

[Tomas et al., 2013] Distributed Representations of Words and Phrases and their Compositionality (negative sampling paper)

#### Math

Singular Value Decomposition Tutorial

#### Reading

[Google's trained Word2Vec model in Python](http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/)

[Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)

[Word2Vec Tutorial - Negative Sampling](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)

### GloVe

#### Papers

[Pennington et al., 2014] GloVe: Global vectors for word representation (original GloVe paper)

[Yin et al., 2018] On the Dimensionality of Word Embedding

[Huang et al., 2012] Improving Word Representations Via Global Context And Multiple Word Prototypes
