# Natural Language Processing with Deep Learning

## Word Vectors

### Word2Vec

#### Papers

[Bengio et al., 2003] A neural probabilistic language model.

[Collobert et al., 2011] Natural language processing (almost) from scratch.

[Mikolov et al., 2013] Efficient estimation of word representations in vector space. (original word2vec paper)

[Rong et al., 2014] word2vec parameter learning explained.

[Tomas et al., 2013] Distributed Representations of Words and Phrases and their Compositionality (negative sampling paper)

#### Math

[Singular Value Decomposition Tutorial](https://davetang.org/file/Singular_Value_Decomposition_Tutorial.pdf)

[The Singular Value Decomposition and Low-Rank Matrix Approximations](https://web.stanford.edu/class/cs168/l/l9.pdf)

#### Reading

[Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)

[Word2Vec Tutorial - Negative Sampling](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)

[Applying word2vec to Recommenders and Advertising](http://mccormickml.com/2018/06/15/applying-word2vec-to-recommenders-and-advertising/)

### GloVe

#### Papers

[Pennington et al., 2014] GloVe: Global vectors for word representation (original GloVe paper)

[Yin et al., 2018] On the Dimensionality of Word Embedding

[Huang et al., 2012] Improving Word Representations Via Global Context And Multiple Word Prototypes

## PCA

<span style="color:red">Understanding and Using Principal Component Analysis</span>

<span style="color:red">PCA and the Power Iteration Method</span>
